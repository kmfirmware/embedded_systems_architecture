
Home> Systems-design Design Center > How To Article	
Application dictates choice: fixed- or floating-point DSP?
Boris Lerner -October 26, 2006

1 Comments
inShare
Save Follow PRINT PDF EMAIL

Without a doubt, many algorithms can benefit from employing a floating-point implementation. The code can be simpler and take fewer cycles to execute than fixed-point implementations. However, these benefits do not really matter that much to end designers. Fewer designers are coding in assembly language, because compilers are getting better; coding fixed- or floating-point implementations in C takes similar complexity. Designers can write floating-point code for a fixed-point processor in C, but doing so causes a significant performance hit. What matters to end designers is the final system's performance, as well as the cost and time to market.

Today's designers have myriad processors from which to choose. Analyzing and comparing all fixed- and floating-point processors is impractical, so this article compares and analyzes a fixed-point ADSP-21531 Blackfin processor and a floating-point ADSP-21375 SHARC. Both of these devices are available for approximately $5 in similar quantities. A first thought might be that, given a similar silicon price, designers should choose the floating-point processor because it can perform the floating-point operations now and in the future should floating-point processing becomes necessary. But a similar price for the silicon does not always translate into a similar price for the end system. For example, performing floating-point operations uses more power than using fixed point for similar tasks. This scenario could mean increased cost in the power-supply design, among other things. In general, each type of application would favor one processor type over the other.

To illustrate, consider applications for a military radar, a mobile television, a professional-audio-effects processor, and an automatic echo canceler in a hands-free portable device. The selection of these applications is not entirely random. These examples illustrate a case in which the fixed- or floating-point processor is a clear winner, in which the answer to the question of which type of processor to use is somewhat unclear and requires more in-depth analysis, or in which the choice can be misleadingly simple. No real-world analysis of this type would be complete without considering the use of on- and off-chip ASIC-hardware coprocessors and FPGAs to offload some of the signal-processing tasks from the DSP. Exploring these options is outside the scope of this article, but such analysis should be part of such a comparison.
Comparisons

Finding the maximal absolute value of a self-ambiguity function lies at the heart of a military-radar application. This function arises as the result of the cross-correlation of a sent test signal with the received echo. The following equation gives such a function:

This equation is the integral of a function against an exponential, and you can compute it using FFT (fast-Fourier-transform) techniques. Floating-point operation is great for computing large FFTs, and no drawbacks exist to using a floating-point processor here. Power is not a major issue as long the heat from the processing can vent so that the processor is not glowing red. The cost of the device is not a major issue, either, because processors constitute a fraction of the cost of the complete system. As a matter of fact, a designer would be unlikely to choose even the sample SHARC device because, for this task, the point is to pack as much processing power as possible into a square inch. Available higher performance floating-point devices with higher processing density would be more appropriate.

Choosing between fixed- and floating-point operation for a mobile-television application is another easy analysis. Mobile TVs have little need for floating-point processing. The bulk of the signal-chain processing resides in standard decoders, such as MPEG-2, MPEG-4, JPEG-2000, and H.264. These algorithms aim for implementation in fixed-point processing; the greater precision and dynamic range that floating-point operation offers is not only unhelpful, but also unusable, because the algorithms are, in general, bit-exact.

Video codecs use some form of DCT (discrete-cosine transform) for the frequency domain. On the surface, floating-point operation appears more suitable for DCT computations. Floating-point computations would produce a more precise DCT. However, these DCTs target fixed-point arithmetic, and they are bit-exact; more precision is simply wrong here.

Much of the video-codec processing resides in the control code, so no need exists for floating-point operations here, either. Looking at the two processors, the Blackfin's video-acceleration instructions, which accelerate the performance of video algorithms, further strengthen the decision in favor of the fixed-point processor. Additionally, power consumption in the mobile-system market is critically important, so floating-point arithmetic is an expensive luxury. This situation clarifies the decision for the fixed-point processor.
Less obvious

The results of the fixed-versus-floating-point comparison for a professional-audio-effects processor are less obvious. Unlike the mobile-system example, this application has no hard limit on power consumption because audio-effects processors usually plug into a wall outlet. So, unless the processor is so hot that it requires forced cooling—and resultant increased cost—to keep it from melting the solder balls, the comparison of power between the two options comes down to the cost of the power supply.

For example, for an effects processor that has to do a La Scala reverberation, a designer would go to the La Scala opera house in Milan, Italy, and measure its impulse response until the final echo fades away. Implementing this impulse response models the hall's reverberation. Or the designer could cheat and download the already-measured impulse response from a Web site. The impulse response for this example is about 2 sec long. Using a 96-kHz sample rate—the middle ground between 48 and 192 kHz—this scenario translates into 192,000 samples in the FIR-delay line. To precisely implement the reverberation, the effects processor would need to use a 192,000-tap FIR. Directly doing a 192,000-tap FIR would require 192,000 multiplications per output sample. At 96,000 samples/sec, this scenario would mean doing almost 18.5 billion multiplications/sec.

For the total processing requirements that this application needs, the designer must multiply this figure by the number of output channels, add any necessary processing, and allow some performance head room for future expansion. After performing all of these steps, the processing requirements for this example come to approximately 100 billion MACs (multiply-accumulate) operations that need to preserve the 24-bit precision throughout the processing. A designer could have 100 processors performing in parallel, but the resulting selling price and size of the box would most likely have a negative effect on its sales. Thus, some tweaking is necessary, and the tweaks available depend on the choice of processor.

Floating-point processing is good for performing large FFTs, so you can implement the FIR in the frequency domain. One commonly used algorithm for this approach is the overlap-add FFT, which basically computes large FFTs from smaller ones. For this example, choose a 1024-point window for computing an overlap-add FFT. The floating-point processor can perform a 1024-point complex SIMD (single-instruction-multiple-data) FFT in approximately 9200 cycles. Thus, to compute 1024 samples of outputs, the application needs to perform a 1024-point FFT, followed by 1024 complex multiplications, followed by a 1024-point inverse FFT.

The multiplication operations are equivalent to 4096 real multiplications, which, on this SIMD processor, take 2048 cycles. To compute 1024 outputs requires 9200+2048+9200=20,448 cycles, or about 20 cycles/output. The total number of required cycles increases to approximately 100 cycles/output after adding cycles for the zero padding necessary to perform an overlap-add FFT, accessing external memory to bring in the impulse response's precomputed FFT, and multiplying by the number of channels. At 96,000 samples/sec, the application needs 9.6 million cycles/sec. In other words, this approach consumes only 10 MIPS of the floating-point processor's processing budget.
Read more In-Depth Technical Features

For the fixed-point processor, the FFT implementation runs into major issues. A 1024-point FFT/inverse-FFT combination has a gain of 1024, or 10 bits. To avoid overflow during the computation of the FFT/inverse FFT, the system must shift down the signal by 10 bits. To preserve the 24-bit resolution, the implementation must keep 34 bits in the arithmetic. Because analog design usually cannot keep up with 24 bits of conversion in its SNR (signal-to-noise ratio), you need to keep 32 bits in the arithmetic to yield 22-bit performance.

A 16-bit fixed-point processor has to perform four multiplications and a few shifts and additions to compute a 32-bit multiplication. It is possible to reduce this number down to three 16-bit multiplications by not computing the last bit, but the math is then 31 bits, and the SNR performance is 21 bits. These values may no longer be good enough for the professional-audio market. In the case of the fixed-point processor, the amount of math increases by a factor of six, and the design needs 60 MIPS of its processing budget. The 60-MIPS budget presumes that this amount is all the processing that needs to take place, which is never the case.

At this point in the comparison, the choice is between the cost of designing a system with the floating-point processor running at 10 MHz and the cost of designing a system with the fixed-point processor running at 60 MHz. At frequencies this low, the dc leakage constitutes a significant chunk of the power, making the frequency difference less relevant and, thus, probably favoring the fixed-point processor. However, as the processing requirements go up, the ac component becomes more dominant, and the factor of six in the frequency starts favoring the floating-point processor. In the extreme case, the fixed-point implementation runs out of MIPS altogether, and the floating-point processor is the correct choice.

In a different approach, a designer could zero all but the dominant terms and implement an approximation of the required reverberation. So, by keeping 1% of the terms nonzero, this scenario becomes a 1920-tap FIR. An FFT cannot do this task at all because an FFT cannot selectively compute terms; it must compute them all. The designer would directly implement this FIR. This situation means that the system implementation needs to perform 1920 multiplications per output sample. To keep 24-bit precision, multiply the 1920-cycle count by six and divide by two; the fixed-point processor can perform two 16-bit multiplies/cycle, or 5760 cycles/output.

The system needs to support 2-GHz operation after multiplying the 5760 cycles by the number of channels and by the 96,000-MHz sample rate. The system must also allow for the overhead and new features. Zeroing 99.9% of the impulse-response terms enables you to do the design at 200 MHz. This frequency is still higher than that of the floating-point processor, even accepting that the filter's performance begins to suffer.

In real life, professional-audio-effects processors usually include more functions than a reverberation. A designer would have to analyze all the pieces of the processing. In many cases, a floating-point processor consumes less power than a fixed-point processor, in spite of the fact that data-sheet power numbers suggest the opposite.

In a final example, an automatic echo canceler in a hands-free portable device often employs an FFT to compute correlation to the reference signal. It may appear that the previous example clearly makes the point about FFTs; however, an important difference exists. In the last example, the result of the FFT/inverse FFT is the actual signal that goes out to the end user's ears and, thus, has to maintain a high-quality SNR. In the echo canceler, you use the FFT only to compute the time delay of the echo. In other words, it computes the parameter that you then use to compute the actual audio. Thus, a designer could get away with doing an FFT in 16-bit precision, and doing so significantly changes the cycle-count analysis. An additional constraint is that the product is a hands-free portable device. This scenario puts a hard limit on the power budget so that a larger floating-point processor simply has too much dc-leakage power to be a consideration.

In the straightforward cases, it is usually clear which approach is best; however, in grayer areas, this analysis can become complex. Couple this complexity with other considerations, such as ASIC-hardware acceleration to offload some of the processing; ease of programmability, which affects time to market; and maximum dI/dt, which translates into cost for the power-supply decoupling and board layout, and this analysis can become an even greater challenge.


